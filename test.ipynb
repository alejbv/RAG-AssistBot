{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar la extensión autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Retriever tools\n",
    "from libs.chatbot import Chatbot\n",
    "from libs.vector_retriever import VectorRetriever\n",
    "from libs.lexical_retriever import LexicalRetriever\n",
    "from libs.basic_document_storage import BasicStorage\n",
    "from libs.hybrid_retriever import HybridRetriever\n",
    "from libs.pdf_loader import PDFLoader\n",
    "from libs.basic_text_splitter import BasicTextSplitter\n",
    "\n",
    "# Prompt Tools\n",
    "from prompts.academic_assistant_prompt import system_prompt as academic_system_prompt\n",
    "from prompts.academic_assistant_prompt import user_prompt as academic_user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "# Load the documents\n",
    "pdf_loader = PDFLoader(file_filter={\".pdf\"})\n",
    "documents = pdf_loader.load_data()\n",
    "# Split the documents in chunks\n",
    "splitter = BasicTextSplitter()\n",
    "chunked_documents = splitter.split_documents(documents)\n",
    "# Store the documents\n",
    "basic_storage = BasicStorage()\n",
    "basic_storage.add_documents(chunked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents to the retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 659 points to 25 centroids: please provide at least 975 training points\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525300944de744eeb5ada7069a1ef2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Create Vocab:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afd62622bc04201a77ab265363d03ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Convert tokens to indices:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353fb5e48c4c4c70821b2f08f30cbd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054112eaac1e49faa17b9b2989a6e56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Create a retriever to handle the retrieval process.\n",
    "# For handling the storage and retrieval of data chunks with semantic search\n",
    "semantic_retriever = VectorRetriever()\n",
    "# For handling the storage and retrieval of data chunks with lexical search\n",
    "lexical_retriever = LexicalRetriever()\n",
    "# A retriever for handling all the retrieval process\n",
    "hybrid_retriever = HybridRetriever(storage_= basic_storage, retrievers=[semantic_retriever,lexical_retriever])\n",
    "# Add the documents to the retriever\n",
    "print(\"Adding documents to the retriever\")\n",
    "hybrid_retriever.add(chunked_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Cuales son las distintas formas de turismo\"\n",
    "query2= \"Hablame de los pasos principales de la organización y el desarrollo de un viaje y los tipos de entidades participantes en ello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53faa1f3b202420f8b4cf2d78b59c1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content=\"I'm working on a project that involves creating a simple chatbot for a customer service department. The chatbot will be used to handle common customer inquiries, freeing up human agents to focus on more complex issues. Here's a list of common questions that the chatbot should be able to handle:\\n\\n1. What are your hours of operation?\\n2. How can I track my order?\\n3. Where is my nearest store location?\\n4. What are your return policies?\\n5. How do I reset my password?\\n6. Can I get a refund or exchange an item?\\n7. How do I contact customer service?\\n8. What are your shipping policies?\\n9. How do I cancel an order?\\n10. What are your payment options?\\n\\nI'd like to create a simple decision tree to help the chatbot respond appropriately to these questions. Can you help me create a basic decision tree for these common questions?\\n\\nHere's a basic structure for the decision tree:\\n\\n```\\nRoot\\n  |\\n  |--- Question 1: What are your hours of operation?\\n  |       |\\n  |       |--- Response: Our operating\", tool_calls=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm working on a project that involves creating a simple chatbot for a customer service department. The chatbot will be used to handle common customer inquiries, freeing up human agents to focus on more complex issues. Here's a list of common questions that the chatbot should be able to handle:\\n\\n1. What are your hours of operation?\\n2. How can I track my order?\\n3. Where is my nearest store location?\\n4. What are your return policies?\\n5. How do I reset my password?\\n6. Can I get a refund or exchange an item?\\n7. How do I contact customer service?\\n8. What are your shipping policies?\\n9. How do I cancel an order?\\n10. What are your payment options?\\n\\nI'd like to create a simple decision tree to help the chatbot respond appropriately to these questions. Can you help me create a basic decision tree for these common questions?\\n\\nHere's a basic structure for the decision tree:\\n\\n```\\nRoot\\n  |\\n  |--- Question 1: What are your hours of operation?\\n  |       |\\n  |       |--- Response: Our operating\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.reply(query1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea764a93eb1c4156a6356b8fe38fc818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: _2Aj0oHwE16tq7b1LKsa6)\n\nTemplate error: syntax error: After the optional system message, conversation roles must alternate user/assistant/user/assistant/... (in <string>:18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/.env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/.env/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/libs/chatbot.py:116\u001b[0m, in \u001b[0;36mChatbot.reply\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    114\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m retrieved_chunkst])\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Last : Generate a response for the user using the given context\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/libs/chatbot.py:107\u001b[0m, in \u001b[0;36mChatbot.submit\u001b[0;34m(self, query, memory, context, role, user_prompt, store, stream)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/libs/chatbot.py:60\u001b[0m, in \u001b[0;36mChatbot._chat\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[Dict]):\n\u001b[1;32m     59\u001b[0m     response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:882\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    860\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    861\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    862\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    879\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 882\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/media/alejbv/Disco1/HDD Old/Carrerra/Mis Temas/DL,NLP,SRI,IA/Proyectos/llms/chatbot/.env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: _2Aj0oHwE16tq7b1LKsa6)\n\nTemplate error: syntax error: After the optional system message, conversation roles must alternate user/assistant/user/assistant/... (in <string>:18)"
     ]
    }
   ],
   "source": [
    "bot.reply(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': \"I'm working on a project that involves creating a simple chatbot for a customer service department. The chatbot will be used to handle common customer inquiries, freeing up human agents to focus on more complex issues. Here's a list of common questions that the chatbot should be able to handle:\\n\\n1. What are your hours of operation?\\n2. How can I track my order?\\n3. Where is my nearest store location?\\n4. What are your return policies?\\n5. How do I reset my password?\\n6. Can I get a refund or exchange an item?\\n7. How do I contact customer service?\\n8. What are your shipping policies?\\n9. How do I cancel an order?\\n10. What are your payment options?\\n\\nI'd like to create a simple decision tree to help the chatbot respond appropriately to these questions. Can you help me create a basic decision tree for these common questions?\\n\\nHere's a basic structure for the decision tree:\\n\\n```\\nRoot\\n  |\\n  |--- Question 1: What are your hours of operation?\\n  |       |\\n  |       |--- Response: Our operating\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.history('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
